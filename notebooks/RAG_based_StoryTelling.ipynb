{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CNielsen94/Random_data_repo/blob/main/notebooks/RAG_based_StoryTelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYA5qJTbAG7-"
      },
      "source": [
        "#Introduction\n",
        "Not long after I started on DBS, I had the idea to create an app that would generate and read custom-tailored stories, and now I get to show you guys a cheap and semi-hacky way to use Retrieval-Augmented Generation (RAG) for this specific purpose. God I love being a nerd."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIWwZshaONor"
      },
      "source": [
        "![](https://raw.githubusercontent.com/CNielsen94/Random_data_repo/main/media/Nerd_is_a_compliment.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlSpEOTCFAiq"
      },
      "source": [
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex21G5UirKOl"
      },
      "source": [
        "#Pip installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDeAHhyWpvo2",
        "outputId": "d5de4010-515a-4ab7-f0f1-fc79d16dc175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.3)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets faiss-gpu\n",
        "!pip install sentence-transformers\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2ermwjUrNOp"
      },
      "source": [
        "#Database setup + fillin' it with good ol' data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPaDP_gFrhyf"
      },
      "source": [
        "First, let's set up a SQLite database to store the text data. Hereâ€™s how you can create a database and a table for storing texts:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOsV3LukCQ7G"
      },
      "source": [
        "This code segment is used to interact with a SQLite database in Python. It starts by importing the sqlite3 module, which allows for communication with SQLite databases. Then, it creates (or opens, if it already exists) a database file named gutenberg_texts.db.\n",
        "\n",
        "A cursor object c is created from the connection object conn. This cursor is used to execute SQL commands. Here, it's used to create a new table named texts in the database, with columns id, title, and content, where id is an integer that automatically increments (PRIMARY KEY), and both title and content are text fields.\n",
        "\n",
        "conn.commit() is called to save the changes made to the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fBg2Nw02rW1p"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "# Create a new SQLite database\n",
        "conn = sqlite3.connect('gutenberg_texts.db')\n",
        "c = conn.cursor()\n",
        "\n",
        "# Create a new table to store texts\n",
        "c.execute('''CREATE TABLE IF NOT EXISTS texts\n",
        "             (id INTEGER PRIMARY KEY, title TEXT, content TEXT)''')\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H4hfO9lsOf2"
      },
      "source": [
        "Now we need to some text to populate the database with. In this case I went through the semi-easy route of just using NLTKs 'gutenberg' corpus. <br> For the sake of context, Gutenberg is an online database of sorts that contain all publicly available written stories, meaning we aren't breaking any of those pesky IP protection rules <br>\n",
        "As I'm sure you're aware by now, the set up steps I am following ***may*** significantly differ from yours depending on what data you want to utilize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiIJ6LcDsEg_",
        "outputId": "f4a237b1-3136-4c32-a1ba-62323289fb0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Get a list of file identifiers for the texts in the Gutenberg corpus\n",
        "file_ids = nltk.corpus.gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x0XGA7ZisIUe"
      },
      "outputs": [],
      "source": [
        "for file_id in file_ids:\n",
        "    # Fetch the text content and title\n",
        "    content = nltk.corpus.gutenberg.raw(file_id)\n",
        "    title = file_id.replace('.txt', '').replace('.zip', '')  # Simple cleanup for title\n",
        "\n",
        "    # Insert the title and content into the database\n",
        "    c.execute(\"INSERT INTO texts (title, content) VALUES (?, ?)\", (title, content))\n",
        "\n",
        "# Commit changes and close the connection\n",
        "conn.commit()\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldMi3nUArqOf"
      },
      "source": [
        "This function, fetch_all_texts_from_db, is designed to interact with a SQLite database to retrieve stored texts, aligning with the Retrieval part of RAG (Retrieval-Augmented Generation). <br>\n",
        "It opens a connection to the gutenberg_texts.db database, selects the content of all entries in the texts table, and fetches these entries. <br>\n",
        "The results, initially in a list of tuples (because that's how database queries return data), are transformed into a list of strings, where each string is the content of a text from the database. <br>\n",
        "This list of texts can then be used as the corpus for retrieval-based tasks in a RAG setup, providing context or source material for GPT (or another generative model) to generate new content based on the retrieved information. <br>\n",
        "Finally, the database connection is closed to ensure resource efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "elps8kuKrWzo"
      },
      "outputs": [],
      "source": [
        "def fetch_all_texts_from_db():\n",
        "    # Connect to the SQLite database\n",
        "    conn = sqlite3.connect('gutenberg_texts.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Select all content from the texts table\n",
        "    cursor.execute(\"SELECT content FROM texts\")\n",
        "\n",
        "    # Fetch all results\n",
        "    results = cursor.fetchall()  # List of tuples\n",
        "\n",
        "    # Convert list of tuples to list of strings\n",
        "    texts = [text[0] for text in results]\n",
        "\n",
        "    # Close the database connection\n",
        "    conn.close()\n",
        "\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp5W5xvDrXGb"
      },
      "source": [
        "#Setting up RAG index for custom story telling:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLUvIBUIGey4"
      },
      "source": [
        "This code segment integrates the retrieval part of RAG and prepares data for GPT. It uses the sentence_transformers library to load a pre-trained model (all-MiniLM-L6-v2) that converts texts into embeddings, numerical vectors representing textual information. <br><br>\n",
        "These embeddings, derived from the database texts, allow for semantic similarity comparison. The FAISS library is used to create an efficient index for these embeddings, enabling quick retrieval of the most relevant texts based on vector similarity. <br><br>\n",
        "This retrieval process complements GPT's generation by providing relevant context or source material."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMbyLNMApzid",
        "outputId": "3264f3d6-9c58-4ed1-e86d-6c5e3a250e3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Assuming you've populated the database with text data, fetch it, vectorize, and index\n",
        "texts = fetch_all_texts_from_db()  # This should come from your database\n",
        "embeddings = model.encode(texts)\n",
        "\n",
        "# Create a FAISS index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(np.array(embeddings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqP9-2Qi4uk6"
      },
      "source": [
        "Since I got some spare credits lying around on OpenAI, I decided to use their API as the backend. The benefit of this, is that I don't have to run the model locally/in Colab, which allows a bit more freedom/creativity in terms of model implementation, as the GPU resources are less scarce.<br>\n",
        "(And it's dirt cheap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JXpJOrLwwqvv"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OTXEVlTpBx-z"
      },
      "outputs": [],
      "source": [
        "with open(\"openai_key.txt\", \"r\") as file:\n",
        "    openai.api_key = file.read().strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJUbO94et3EX"
      },
      "source": [
        "#Functionality setup\n",
        "I'll set up some extra functions to tie this application together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj3HQvXqG_U_"
      },
      "source": [
        "\n",
        "First we want to retrieve all the texts in the FAISS index.\n",
        "This function, **retrieve_texts()**, embodies the retrieval aspect of RAG. <br><br>\n",
        "It uses embeddings to find texts most similar to a user's query within a corpus. <br>It encodes the query into a vector, searches the pre-indexed embeddings for the closest matches, and retrieves the corresponding texts based on their index positions. <br>\n",
        "This selection serves as context or inspiration for subsequent text generation, enhancing GPT's outputs with relevant, query-specific information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OXHOCMlgp0PD"
      },
      "outputs": [],
      "source": [
        "def retrieve_texts(query, model, index, texts, top_k=5):\n",
        "    # Encode the query using the same model\n",
        "    query_vector = model.encode([query])\n",
        "\n",
        "    # Perform the search\n",
        "    D, I = index.search(np.array(query_vector), top_k)\n",
        "\n",
        "    # Retrieve the texts corresponding to the indexed IDs\n",
        "    retrieved_texts = [texts[i] for i in I[0]]\n",
        "\n",
        "    return retrieved_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTtHTaGxHXuH"
      },
      "source": [
        "This function, **generate_story_based_on_texts()**, integrates the Generation part of RAG. <br>\n",
        "It takes user-defined elements (like setting, character) and retrieved texts to craft a detailed prompt, then uses GPT-3 to generate a narrative. <br>\n",
        "This approach combines structured storytelling with dynamic, AI-driven content creation through prompt engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nxyOs4Hdp0NC"
      },
      "outputs": [],
      "source": [
        "def generate_story_based_on_texts(retrieved_texts,\n",
        "                                  setting=\"a mysterious forest\",\n",
        "                                  character=\"a brave young explorer\",\n",
        "                                  objective=\"find the lost city of gold\",\n",
        "                                  tone=\"adventurous\",\n",
        "                                  style=\"light-hearted\",\n",
        "                                  model=\"gpt-3.5-turbo-instruct\",\n",
        "                                  max_length=1000):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Construct the initial prompt using the user's input\n",
        "    initial_prompt = f\"\"\"\n",
        "    The setting of the story should be {setting}. \n",
        "    The main character is {character}.\n",
        "    The main characters objective should be {objective}. \n",
        "    The tone of the story should be {tone} and the style of the story should be {style}.\n",
        "    \"\"\"\n",
        "\n",
        "    # Combine the retrieved texts with the initial prompt\n",
        "    combined_text = f\"\"\"\n",
        "    You will be provided with the task to generate a new, original story. \n",
        "    The story should contain the following elements:\n",
        "    {initial_prompt}.\n",
        "\n",
        "    You should use the following as inspiration, without directly mentioning any of it.  \n",
        "    {' '.join(retrieved_texts)}\n",
        "    \"\"\"\n",
        "\n",
        "    # Call GPT-3 to continue this story\n",
        "    response = openai.completions.create(\n",
        "        model=model,\n",
        "        prompt=combined_text,\n",
        "        max_tokens=max_length,\n",
        "        temperature=0.7,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    # Extracting the generated text according to the new response structure\n",
        "    story = response.choices[0].text.strip()\n",
        "    return story"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxDrHbWIHvWK"
      },
      "source": [
        "The **refine_texts()** function processes retrieved texts to extract and concatenate a specified number of leading sentences from each, enhancing clarity and relevance before they're used for story generation. <br><br>\n",
        "This acts as a form of summarization, focusing on the most significant parts of each text to provide GPT-3 with a distilled context, hopefully resulting in more coherent and contextually relevant narrative outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F-hoAtNzu6dD"
      },
      "outputs": [],
      "source": [
        "def refine_texts(retrieved_texts, max_sentences=3):\n",
        "    # This function could be enhanced to include summarization\n",
        "    refined_texts = []\n",
        "    for text in retrieved_texts:\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        refined_texts.append(' '.join(sentences[:max_sentences]))  # Take only the first few sentences\n",
        "    return ' '.join(refined_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zha-4wbUI2vW"
      },
      "source": [
        "Finally we combine all the functionalities into one: <br>\n",
        "\n",
        "The **create_story_from_query()** function combines retrieval, refinement, and generation steps in RAG: it fetches texts related to a query, condenses them to essential content, merges this with a narrative starter, and employs GPT-3 to unfold a story, leveraging structured inputs for creative output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZBjckMzAp0It"
      },
      "outputs": [],
      "source": [
        "def create_story_from_query(query, initial_prompt=\"Once upon a time\"):\n",
        "    retrieved_texts = retrieve_texts(query, model, index, texts, top_k=5)\n",
        "    refined_text = refine_texts(retrieved_texts)\n",
        "    combined_text = f\"{initial_prompt} {refined_text}\"\n",
        "    story = generate_story_based_on_texts([combined_text])  # Adjusted to accept combined text directly\n",
        "    return story"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNxBVB2b6Vi1"
      },
      "source": [
        "#Let's try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL5hgRGrp0GZ",
        "outputId": "a70abf23-eb29-42f3-ab54-0c67f47f677e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After\r\n",
            "that he yawned until it seemed as if his jaws would crack, and started\r\n",
            "off towards the Laughing Brook.\r\n",
            "\r\n",
            "As he drew near, the sound of the Laughing Brook grew louder and louder.\r\n",
            "It was the same merry, bubbling laugh that had greeted Buster Bear every\r\n",
            "morning just as long as he could remember. It was coming from the\r\n",
            "smooth, black pool in which the Laughing Brook ended its long journey\r\n",
            "down the side of the Green Mountains. Buster loved that pool. He loved\r\n",
            "to watch the speckled trout darting to and fro in its clear depths. He\r\n",
            "loved to watch the flies dancing above it, and the little birds\r\n",
            "splashing in it. But most of all, he loved the taste of the trout who\r\n",
            "were foolish enough to let him catch them.\r\n",
            "\r\n",
            "With a loud splash, Buster plunged into the pool and began to swim\r\n",
            "around in search of breakfast. Suddenly he stopped and turned his head,\r\n",
            "for he\n"
          ]
        }
      ],
      "source": [
        "# Generate the story\n",
        "query = \"adventure in the mountains\"\n",
        "story = create_story_from_query(query)\n",
        "print(story)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1FSSqoFJGsB"
      },
      "source": [
        "#Can we make this even more nerdy and fun? (ALWAYS!)\n",
        "Let's add some custom Text-To-Speech to wrap this notebook up. There are lots of models that allow you to mimic a voice with very few (or even just a single) examples. \n",
        "**(This part of the notebook isn't fully implemented yet. I need to make sure the data loading etc for the voice clip functions correctly)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF2QvkogK-FC"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/coqui-ai/TTS.git\n",
        "%cd TTS\n",
        "!pip install -r /content/TTS/requirements.txt\n",
        "\n",
        "!pip install coqpit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -O voice.wav 'https://github.com/CNielsen94/Random_data_repo/raw/main/media/380339__scottemoil__male-deep-voice-lines-of-dialogue.wav'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from TTS.api import TTS\n",
        "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fSwtMkcK9Pu"
      },
      "outputs": [],
      "source": [
        "# generate speech by cloning a voice using default settings\n",
        "tts.tts_to_file(text=story,\n",
        "                file_path=\"output.wav\",\n",
        "                speaker_wav=[\"/content/voice.wav\"],\n",
        "                language=\"en\",\n",
        "                split_sentences=True\n",
        "                )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO9Xi4TY+KYmBkYrhejFq9u",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
