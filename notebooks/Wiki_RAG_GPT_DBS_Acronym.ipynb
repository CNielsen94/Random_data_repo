{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CNielsen94/Random_data_repo/blob/main/notebooks/Wiki_RAG_GPT_DBS_Acronym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsTsUBNaVxMc"
      },
      "source": [
        "#Let's refresh our memories with some questions:\n",
        "1. What does GPT stand for, and what are the possible benefits of utilizing these?\n",
        "2. What is RAG? Why is it relevant in terms of GPTs?\n",
        "3. Can you give some example use cases (silly or serious) of RAG?\n",
        "4. What are some of the differences between prompt engineering and fine-tuning?\n",
        "5. Why is one sometimes preferred over the other?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlpU5xGxXYzQ"
      },
      "source": [
        "##My ideas about it:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEU-VW2NXrHU"
      },
      "source": [
        "**What does GPT stand for, and what are the possible benefits of utilizing these?**\n",
        "\n",
        "GPT stands for \"Generative Pre-trained Transformer.\" It is a type of artificial intelligence model designed for understanding and generating human-like text. The benefits of utilizing GPT models include: <br>\n",
        "- Natural Language Understanding: GPT models can comprehend complex language nuances, making them useful for a variety of NLP tasks.\n",
        "- Content Generation: They can generate coherent and contextually relevant text, aiding in applications like content creation, chatbots, and more.\n",
        "- Scalability: GPT models can handle different scales of language tasks, from simple text generation to complex question answering.\n",
        "- Customization: While GPT models are pre-trained, they can be fine-tuned for specific industries or applications, making them versatile tools.\n",
        "\n",
        "**What is RAG? Why is it relevant in terms of GPTs?**\n",
        "\n",
        "RAG stands for \"Retrieval-Augmented Generation.\" It is a framework that combines the powers of language models like GPT with information retrieval systems. RAG is relevant in terms of GPTs because:\n",
        "- Enhanced Information: It enables GPT models to access a broader range of information beyond what they were trained on, by retrieving external documents or data and incorporating that information into the generation process.\n",
        "- Improved Accuracy: By using retrieved information, RAG-enhanced GPT models can provide more accurate and contextually relevant answers.\n",
        "- Versatility in Applications: This approach is particularly useful in applications where the answer depends on the most recent or specific information, such as question-answering systems.\n",
        "\n",
        "**Example use cases of RAG (silly or serious):**\n",
        "\n",
        "- Serious: Creating a medical chatbot that retrieves the latest research articles to provide up-to-date health advice.\n",
        "- Silly: Designing a story generator that pulls in random facts from the internet to create bizarre and humorous tales.\n",
        "- Mixed: Developing a cooking assistant that suggests recipes based on the random ingredients it finds from various online sources.\n",
        "\n",
        "**Differences between prompt engineering and fine-tuning:**\n",
        "\n",
        "- Prompt Engineering: This involves crafting the input text (prompt) to a language model in such a way that it elicits the desired response. It doesn't require changing the model itself but understanding how to effectively communicate with it.\n",
        "- Fine-Tuning: This process involves continuing the training of a pre-trained model on a new dataset specific to a particular task or domain. It customizes the modelâ€™s parameters to better perform on tasks that differ from the original training data.\n",
        "- Key Differences: Prompt engineering is about finding the right questions, while fine-tuning is about teaching the model new answers.\n",
        "\n",
        "**Why is one sometimes preferred over the other?**\n",
        "\n",
        "- **Prompt Engineering is often preferred when:**\n",
        "1. Access to the model's internals is restricted.\n",
        "2. There is a need for quick, cost-effective solutions without the computational resources for retraining.\n",
        "3. The task can be accomplished by creatively formulating prompts.\n",
        "- **Fine-Tuning is preferred when:**\n",
        "1. There is a specific, recurring task that the base model does not perform well on.\n",
        "2. There is enough labeled data to retrain the model effectively.\n",
        "3. The goal is to significantly improve performance on a narrow task or specific domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvdh3nBmB87l"
      },
      "source": [
        "##**Some ways of thinking about using RAG and GPT in a project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ko2_lzFS8_1"
      },
      "source": [
        "**GPT Models (Generative Pre-trained Transformer):** <br>\n",
        "GPT models are a type of language model that use deep learning techniques to produce human-like text. They can be used for a variety of applications such as text generation, translation, summarization, and question-answering. In the context of business data science, GPT models can be leveraged for:\n",
        "\n",
        "**Natural Language Processing (NLP) tasks:**\n",
        "Analyzing customer feedback, generating product descriptions, or creating chatbots. <br>\n",
        "**Data Analysis and Interpretation:** Summarizing reports, generating insights from data, or explaining complex datasets in natural language. <br>\n",
        "**Predictive Analytics:** Forecasting market trends, consumer behavior, or financial indicators based on historical data. <br>\n",
        "\n",
        "**RAG (Retrieval-Augmented Generation):** <br>\n",
        "RAG is a hybrid model that combines a retrieval-based approach with a generative model like GPT. It first retrieves relevant documents or data snippets based on the input query and then uses this information to generate a response. RAG is particularly useful for applications where the response requires external knowledge not contained within the model itself. <br>\n",
        "Some examples of what RAG can be used for:\n",
        "\n",
        "**Enhanced Question Answering:** Providing detailed answers to business-related questions by retrieving and synthesizing information from various sources.<br>\n",
        "**Content Creation and Summarization:** Generating reports, summaries, or articles that incorporate data and information from multiple business documents.<br>\n",
        "**Decision Support:** Offering recommendations or insights by retrieving and processing relevant business data and trends.<br>\n",
        "\n",
        "**Integrating GPT and RAG into Your Project:**<br>\n",
        "To utilize GPT models and RAG in your project, consider the following steps:\n",
        "\n",
        "**Define the Problem:** Clearly define what business problem you are solving. Is it customer service automation, market analysis, content generation, or something else?\n",
        "\n",
        "**Data Collection and Processing:** Gather and preprocess the relevant business data. For RAG, you will also need a dataset or a corpus from which the model can retrieve information.\n",
        "\n",
        "**Model Selection and Training:** Choose the appropriate GPT and RAG models for your task. You may use pre-trained models from libraries like Hugging Face's Transformers and fine-tune them on your specific business data.\n",
        "\n",
        "**Integration and Deployment:** Integrate the models into your data processing pipeline. Ensure that the RAG model can access and retrieve from your business data corpus effectively.\n",
        "\n",
        "**Evaluation and Iteration:** Test the system's performance on real-world business queries and tasks. Collect feedback and iteratively improve the model's accuracy and relevance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24SIyRJiZQ6n"
      },
      "source": [
        "#Aight, enough with wordy nerdy stuff, let's code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yZW9G4w71yF"
      },
      "source": [
        "## Logical flowmap\n",
        "**So let's go over the logical flow of this app. I know it seems super simple from this flowchart, but we gotta start somewhere ya know**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrv01gvUBhfy"
      },
      "source": [
        "![](https://raw.githubusercontent.com/CNielsen94/Random_data_repo/main/media/SuperSimple.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJa-JL9K8Vv6"
      },
      "source": [
        "**I bet that scared ya for a second ;)** <br>\n",
        "Not saying that I'm an artist, but look at the actual masterpiece I created:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1oHQJ1l8bLl"
      },
      "source": [
        "![](https://raw.githubusercontent.com/CNielsen94/Random_data_repo/main/media/ImNotAnArtist.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuGE9nu2ewoF"
      },
      "source": [
        "##Pesky installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R1XgWzwebXT",
        "outputId": "b87cee1e-78ae-4b4e-f634-ff645b06ccff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone-client sentence-transformers requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_K8VvPHkecpj"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "import requests\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmqLAZEVhcEw",
        "outputId": "3f8f26d4-7ee3-4f5e-a3cb-f41c72c45c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF-GQHWLezYB"
      },
      "source": [
        "## Setting up vector storage (pinecone index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRFOP0rZAb2H"
      },
      "source": [
        "I had to read up on this myself, since it's been I don't know how long since I used it myself, but when you set up Pinecone indexes now, you gotta choose between a Podspec and Serverless index. In the case of Pinecone, the latter is a paid service, so we'll run with Podbased. If you're interested however, you can glance the main differences here: <br><br>\n",
        "**PodSpec (Pod-Based Deployment):**\n",
        "\n",
        "**Resource Allocation:** In a pod-based deployment like the one indicated by PodSpec, resources are allocated as containers or pods. This can provide more dedicated resources compared to serverless, leading to potentially better and more predictable performance.<br>\n",
        "**Cost:** While offering more control and potentially better performance, pod-based deployments typically have a fixed cost based on the allocated resources, regardless of usage. This means you pay for the resources even when they are idle.<br>\n",
        "**Scalability:** Pod-based deployments can be scaled, but this typically requires manual adjustment or setup of auto-scaling parameters. The scaling process is not as instantaneous as with serverless.<br>\n",
        "**Use Cases:** Ideal for applications with predictable workload patterns or when consistent performance is critical. <br><br>\n",
        "**Serverless (Serverless Deployment):**\n",
        "\n",
        "**Resource Allocation:** In a serverless deployment, the underlying infrastructure is abstracted away, and resources are dynamically allocated based on demand. This means your application can scale automatically without needing to manage servers or containers.<br>\n",
        "**Cost:** With serverless, you typically pay only for the compute time you use, which can be more cost-effective for sporadic, unpredictable workloads or applications with varying levels of traffic.<br>\n",
        "**Scalability:** Serverless services can automatically scale from zero to handling potentially thousands of requests per second without manual intervention. <br>\n",
        "**Use Cases:** Ideal for applications with variable traffic, microservices, and for developers who want to focus on code rather than managing infrastructure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0VP6gQt2RWvQ"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, PodSpec\n",
        "\n",
        "# Pinecone setup\n",
        "# Load Pinecone API key from a file\n",
        "with open(\"pinecone_key.txt\", \"r\") as file:\n",
        "    pinecone_api_key = file.read().strip()\n",
        "\n",
        "# We instantiate a pinecone object using our API-key, and save it to the variable pc. This variable is where all the index magic happens!\n",
        "pc = Pinecone(api_key=pinecone_api_key) # We instantiate a pinecone object using our API-key, and save it to the variable pc. This variable is where all the index magic happens!\n",
        "index_name = 'wikipedia' #provide a name for our index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUgLYWtOGotM"
      },
      "source": [
        "Quick note on Pinecone: <br>\n",
        "If you don't run some sort of check as to whether or not your pinecone index already exists, and you've already instantiated it, it will cause an error saying that an index of that name already exists. <br>\n",
        "This is why I've set this little if-statement up, just so that I can avoid that runtime error. In your projects, you can simply create the index either manually on the website, or through code which you then comment out to avoid runtime errors (or check the existing indexes like this)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uUroFSqZGoKF"
      },
      "outputs": [],
      "source": [
        "# Initialize or connect to the Pinecone index\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,  # Matching the Sentence Transformer model output embedding vector dimensionality\n",
        "        metric=\"cosine\", # This is our metric for measuring similarities\n",
        "        spec=PodSpec(environment=\"gcp-starter\")\n",
        "    )\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzCSOPguMIgr"
      },
      "source": [
        "READ THE DOCUMENTATION FOR PETES SAKE! - https://docs.pinecone.io/docs/upsert-data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkag1Xvae6tu"
      },
      "source": [
        "## Let's set up a data retrieval pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQg_anbzAsu9"
      },
      "source": [
        "In order to put our data into our pinecone index, we need to turn them into vectorized embeddings. We do this using a sentence transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwe3gkxdegEV",
        "outputId": "f5cdec43-6a8a-404a-da44-6b41cb698f2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Initialize Sentence Transformer Model\n",
        "sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5kdoEiuf2mg"
      },
      "source": [
        "First we set up a a function to ping the Wiki API. In this case I am pulling the content of the specific content page, as well as the last time it had a revision. I do this so that I can keep my index updated by comparing the metadata. <br>\n",
        "Ideally you would probably want this to be a separate workflow (self-updating DB), but I'm gonna do this just to show you how to set up a pinecone index and populate it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e1r9SyxNejO4"
      },
      "outputs": [],
      "source": [
        "def fetch_relevant_wikipedia_content(search_term):\n",
        "    # First, search for related article titles based on the search term\n",
        "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    search_params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": search_term,\n",
        "        \"format\": \"json\",\n",
        "        \"srlimit\": 5  # Adjust based on your needs\n",
        "    }\n",
        "    search_response = requests.get(search_url, params=search_params).json()\n",
        "    search_results = search_response.get('query', {}).get('search', [])\n",
        "    titles = [result['title'] for result in search_results]\n",
        "\n",
        "    # Now, fetch the content for each of the found titles\n",
        "    content_data = []\n",
        "    for title in titles:\n",
        "        fetch_params = {\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"titles\": title,\n",
        "            \"prop\": \"extracts|info\",\n",
        "            \"exintro\": True,\n",
        "            \"explaintext\": True,\n",
        "            \"inprop\": \"url\"\n",
        "        }\n",
        "        fetch_response = requests.get(search_url, params=fetch_params).json()\n",
        "        page = next(iter(fetch_response['query']['pages'].values()))\n",
        "        if 'extract' in page:\n",
        "            content_data.append((page['title'], page['extract'], page.get('lastrevid', 0)))\n",
        "        else:\n",
        "            content_data.append((title, 'Content not found.', 0))\n",
        "\n",
        "    return content_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xk4IKHlXJp5"
      },
      "source": [
        "Now that we can fetch our data, we could technically implement RAG right away by just having the model constantly asking Wikipedia for information to augment the text generation. <br>\n",
        "However, since part of your assignment is to utilize vector databases/indexes, I've decided to completely over-engineer this thing. I've done my best to comment out the individual code pieces, so that you can read through and (hopefully) understand each part.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYKz_rxsYDPL"
      },
      "source": [
        "**\"Pro\" tip:** Note that I've commented out a bunch of print() statements inside the function. You can utilize print() statements inside functions to make sure your inputs/outputs and variables are formatted correctly. Since Python (including any defined function) runs line by line, these statements will work regardless of the rest of the function failing. When you're done debugging either delete or comment them out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0Qs4yqNqO8D8"
      },
      "outputs": [],
      "source": [
        "def add_or_update_page_in_pinecone(article_data):\n",
        "    \"\"\"Add or update a Wikipedia page in the Pinecone vector database.\"\"\"\n",
        "    # Unpack the tuple into variables\n",
        "    title, content, revision_id = article_data\n",
        "\n",
        "    # Format title for Pinecone\n",
        "    formatted_title = title.replace(' ', '_')\n",
        "\n",
        "    # Check if content is valid\n",
        "    if not content or content == 'Page not found.':\n",
        "        print(f\"No valid content for '{title}'. Skipping update in Pinecone.\")\n",
        "        return\n",
        "\n",
        "    # Generate content embedding\n",
        "    content_embedding = sentence_transformer.encode([content])[0].astype('float32').tolist()\n",
        "\n",
        "    # Prepare the data for querying Pinecone to check if an update is needed\n",
        "    query_results = index.query(vector=content_embedding, top_k=1)\n",
        "    if query_results[\"matches\"]:\n",
        "        existing_title = query_results[\"matches\"][0][\"id\"]\n",
        "        if existing_title == formatted_title:\n",
        "            fetch_response = index.fetch(ids=[formatted_title])\n",
        "            existing_record = fetch_response['vectors'].get(formatted_title, {})\n",
        "            existing_metadata = existing_record.get('metadata', {})\n",
        "\n",
        "            if existing_metadata.get('revision_id', 0) >= revision_id:\n",
        "                print(f\"No update needed for '{title}'.\")\n",
        "                return\n",
        "\n",
        "    # Upsert the new or updated content and metadata into Pinecone\n",
        "    try:\n",
        "        index.upsert(vectors=[(formatted_title, content_embedding, {\"revision_id\": revision_id, \"content_summary\": content[:100]})])\n",
        "        print(f\"Page '{title}' (formatted as '{formatted_title}') added or updated in Pinecone.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while upserting '{title}' (formatted as '{formatted_title}'): {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_jD4bg_sAFg"
      },
      "source": [
        "# Setting up the API\n",
        "(Since I've hit the GPU backend limits on Google, I've had to continue the notebook using the same API as the other example, but I've left in the local model code equivelant to give you an idea of how to do it. The workflow doesn't differ much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs15-jtoh9aI"
      },
      "source": [
        "But this is dirt cheap as well. During the entire development of this notebook (which took way too long, but was great fun), I've spent about 0.04 out the 10.00$ i had left.\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAy0AAAA1CAYAAABMfQRtAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAABI0SURBVHhe7Z3PbxRHFseL/SMGo4WDpT3GkWIuGCkXyCreY4yIhFGMVjnmFsUnIAcCJ0e5cYxWJoJIiUyOa7SBy0o2F4zEcIzkQ1jhzD/hrVc/uqu6qn/MeBx64PORWh739FR3z/R7/b71XlWfONQoAAAAAACAnvIX9xcAAAAAAKCXIFoAAAAAAKDXIFoAAAAAAKDXIFoAAAAAAKDXIFoAAAAAAKDX1M4e9uzZM/cKAAAAAAAg5uzZs+7V8cOUxwAAAAAA0GsoDwMAAAAAgF6DaAEAAAAAgF6DaAEAAAAAgF6DaAEAAAAAgF6DaAEAAAAAgF6DaAEAAAAAgF6DaAEAAAAAgF6DaAEAAAAAgF6DaAEAAAAAgF6DaAEAAAAAgF6DaAEAAAAAgF6DaAGAY+BAPfj0hLr044H7HwCgnoMfL6kTnz7QngMAIA+iBQAAAAAAes1MiZbdb06oEyeCpaVXJtk+WN54D/CrB+rSn3kcT++Y877z1P0PcExYuzulrv6s1MPVU9bmvtl178YkNlqzXYSznfJzl9SDV+69hF11J9q22QbM8UR+xWaMws+HS1d7Gv26odbX19W9l25FlpF6vLGu1n8Yuv/95zbU45FbEfLynmmz9v0j445n47F+BXBMuHvTqdWHSv18VZ0ytnVHW26KycYE9tctMzOBDwi3r/NJiR+qb3ciPxfhbHH9niq9Q4bRY7VR8TPDH+o/5/1Sa7uT4nxUs98D6M4bES3W8TQFGlWs0zn/tfvXYxxc3rlNi/GP9V3EBXZvMLXP79QHauxU+Pp85fep2Va2a7iOzO985qrS4U3AQ3X1TCZgMMHQeXXD/eu5sVTTWaCDkG9zx35kRmr4XIf9g2W1/J5blePlttruqg4kONnUYYZuc10HMxcHbn0EogP6jQnml6oWKtxQ57V/KO3U3mOMsAlpiwHG8gHdfVLeD+XanczPJYyGak9cyMfLasGtyjF8tN3d1rWg2Hikt35/TW1srOXbRXRAz5iJTMvBj99ap3NrRx0eHhbL6wcreuUN9W02W3Gg9qXroPIZv2xdmbObvSucu27O+/o59z/AlPF2uvLgtb7WXqv7l/1rbXO/31crlxe1MLC2uvuNCyQu31evC7u0n5FA5IsaUfGFC1pu75a2fLh726y7sRQGLzpYcMFQcQyyyHHodQ9Xv7ACKuwtzQQhSu2r/Z8rbQRLJ3vyAccHCyrRFkW2RC8iQroggmVDByciWNYvpm0CzAK+k8D5AHM/L/yB+IIVtajtTzzBwY9fmMyttny1E9jfzi1ZpwVONnPR0Qc4uvukXfW98UMr6v7vfju9OD/0cPX7wg9N5OcyjF7saTEyUIvvp9ZuMyl2uffCrWxD/I74GxEsnzXJIIB+MQOi5UA9+UU7CDH6m0tunWXuylfahWkn8du+XZFh5W/z7hUAHCf7v1k7vZvrEDi9qrZ+uq5Wz8l7u+qJ6XnUAchPq6rcek6t/rRjbTq48XsO/rtlRIUEIJFY0II86cB4+sQGC7d24g4KOQ4TXDxUV/9V2z+bsDg/eSdHU8AxPkN1TwSLWlBrCBaYZV7tayvUgf93oQ/wiC/YUtevLOlXLgYwIuG6CqOApZtOAHz9bZplH8sHjOGT9HHv6T8rD+6q1dOywqH9kBVRe2rfHMtkfi7FZ2oX1cI0DD7M0iJYYMboIFrSmu47TzPlQMGYiWr9Zlm2YVOlNsVrSzq6lPRc+O5QHUZGb/E9u7f/HosZi+0hVeqJ2Wd5PF3LyZqO1b6X1qVW1wffU9Gjm56vLW2S9+xSN86l+r3WbZclGdOSO7a6dt15BUvRjmnXjl+wqXr9fnFd1J9/XTlX8/py/+XvONk1BdPGZTa79B66YGLlwedRAGJZUheiG7/HBy631VcZUTT34SXbe+o6MHb/Y/ag7v8z4xvOXTABgxruqwMJYIoeUBtIRLgARe1X6tdD39dIS8DxnpRmbNhlfblFhGjBYmrPtWCpK+fwmAzOhi03G22bOvdqmVhZz+6WYCxNE/k6eDm2oK1knE1Qqubq7v22G79GG2qqbVGe8jZysC+WpX32ly229OqJ2pL7y62vYpFgmFMXPjGWX/EXY/gA+TuOTzJiK8/834JjmcjPZWjK1GoWPnP+Qy/rH7eoms5ZWmevLvs73MzZYZvN11F+LrT9Vn8UlKqF2aXceJykLUpk3xpaRIsEhC4gDbixlK7zSE1ntX6ztoa8E3NqrnBUcfAsgWrS61rh4epV29tSILWyf3JQK8F8tvREju9UUqdrBi9Hgsied/V7Ndt1Dp5qyBybtBsLzTHGBeRoOP92rPCpfkdv5HeEBubU6ne+7EIG35cD8avXiQ1W6rMXS383fZCVm7nrhLg8r7K509MX1CXpcTVBiBNQalHNJ0GO4AKGn3WbdkUrN7Qfia5AI9A7dIC0BBzdkSBCbs4DtbzeIlhasQGJqWcPeSFBQfOAXAkW5HML1yRIcsdhgonq50ZqWwdRSRAjAspkikpGjzaCYMgLsxgJmlJxA7PM3JW7RZmUdHYZHy+vq/c0JxLqqiZ8h8Xefvip8XzAWD7JlVrf/dC8FeGzzZ/rmGQyP5cyvUyttq1pZWlrbb7FTk2Hhf6cGYunhcVHchQN/ijTkSK+IC6Dk/MqRYkIlqQt03HT7NtgNmgULc1jSeqo1HkW9aOSBl1S14vP++22TO9J2pMeZgXqqS0N8z2kKq6BtfuO08LVLIIVAvXHOhHFd5i2EdXn+97eIN1d1MVGv0NZF/u9+54m/Q6jdv34gP+476dIhcc1/Tu3XL2xceDuWHzdbiYr1nT+jTz93grkqCZYlh21s3tXzb+an+7vBJPjsha2RKLEipijdFy8QXyvauX687X0+fF0JdMKOPY2XdZEt7b3oiEo8JgMzrpalt26AGHDBSqjXzdtW369W9belw8O1XZN0CGCRYIFESxrxYQCOuD4tw4FKm3Zfesg5lEmTDADf91216z8Gj53241G6g/9Z/CxDmKC9tb093dSv9PhzGFmkDIpbU/unlPgRMyRO+SOmbnTlbucG6Nz+8tcudukTKs07A8tKHzQPlTPWzOXA3VxvbRP20nh7V6LBJeB8evN4jLFo0fbeXFQl+VxE5BEbcki+36xncneSMeN3875uNGeGrrt/ngtLyQbHbQlxyb+jYztzNMoWkyvgQT9yVgS10OSIanz1IHMXRNQdkiDtmKFhA8cTKAqs3DkBuEVZR9xDezclS0bcHz9pL2XdGqk32GBDubjTJE+R1dva78v12MkQVPURlkXWwiMiagc27nP7W/r0+an57U4ScXh0k1fb9yFhvNvwab49ecTIbSkls7NmQX6xdJNsTsrZEXsehETZ/BmBNerWhXivpb+4S9PGgKraQUcI4nlrVjQN944MzE+xU290tu68JkNAEZaQLj7f0FesGhcJqkoQSsWJ7JePK8EMHq/YR39e8s26DjQ5yj/D0Sc6OZei3QpkWNb++io2SroJc7GzP1c7nNexIh4yQ6w7yFS/nzmqlrcnfJkN1PM1I5GEuxbGx9uHmGa9KBjIfIFg4tq3YicjCg6qC9L8x0WvgStWIwwSjtpBh9fC2ZL1OLqH7LPkfqf2+zkKXnzD+MzC+TYPltTC02zN8JM0CBafHp1PHKp0Ll5E/Y2ihYRE16M+KXN+CcVILbu1IuoWAiZJZcpOAp1JS2abMpbCwVzhCbd7cpiFuYzx6TbDQTGJN9h07FZ7Pezo2TK2jKDkxWKdbTuow4v2Cb9PPQBI2JcxlWuaesP/PWd4mvR47IOd63XlXT5undjJ3Nq3sTFdR0lboDska4rt4+mErOpBRylWFiQG6/+f7g5aamDDl7kax/81YiDmIEaiJMZ/c8EJQVakGy7coxkv3rbsWKf7H5DbA/pmrL168XScbwNvAWE2Xsz7szeD+uqKvwEHXHsMZ4PmMwnOWRc55ktden3+H57pDYd0ysNs4Ll4kAH+dckIzJS25sTjvNwNn9yLnNM2r6NZDiIWy6mYta+ZDPK5Dp/NAbZ/QYMPlpXG+uLam8j8B+dx9tA32kQLX6AWzqdYDn9YErOQG1tZ7Nx1uIGkOdLS+qElSv5qkkv2wxSXa3rn0vWEbuSFOuEXbDmMx8RTYJmutjec7/oG8qwJsM1VZoCw121O2u99m81duxRp0yKGwSbnznHz7hTtc/SH+VKsoqZxVwngK8Xz84Q5mcVarMbP3lE9jpvF9T2Rr2glk3d9lEYqL8WTUiGRIKOuI67OzXCxFAnaFw5Rm6/LkhJSjuKZbLxN+HgYlMCcpCvb4fZxZQzd7mH+PFquRnCtB36mcWq9/OxfMBEPkm/Y54zo9ROriR5wjZLhmrbPEdlueY5TONwUpKYFp8RkezoJDblbL4qTAx1gsaVhKaZYuePkvLScrHjXsZEzjFs59rJ/Bg7mDkay8P8lML2wXDaON2SDoouqc59LqLDbj+hSCgMPzOg14unWxeiEjDJDthBdumgPnEyZkB78plxcEIicqIStKUD1lvR320U6EmgZOaW907YB+7VVHm5v/zsaVNCjqfuxpIVUt3w2bfohlJcKyX2xqOFc/I7nlfnjzTBA0wVN/ZIJmhIhYu+Vr+0A9mtEPcz51R/1/Kazs24U8wQVi0zK66bYGYxPzuQtq/oGgnsKzurUEgRLFUFuhynnWhg5ZMLNcJnqJ5LduL9DyYK2hvRN+RrMktQ0mvZDVs+kYqe4Q+2pCvJDPnyttx+BwtqUa/KlZuMovqMjkjde10g5UvI4C3APeukpry7uLe7zKmfIezqmXjyi91v3KRAuZnFxvIB4/okWa9jCSVjNeMS9JLJ/FzBS1tauaDtceq8t2bHsL24N36pqS/hrJapiu2akq4F9UGlDMufQy5TbN7LCSjtPyaxd/FjeXFSlpDB7NIoWsTorvs0bcDt3XRdiTiWUuD4p93e3q0adrldc+Dpx3jYYKVoVy82UPGOxw+ot05t6aYb0O4H9bnFzsA17hiL6rGGTtS3XT+jWhsS6Pnj8w+4C8cGFeciDt5v5/d32c5Ucjxo5yrBZrTfct9JwFZ81x1m9QpuKEW77lqJ8GNscr+jPvf4mSBdrymYOlLS4UrA7PVsrxFrsz6wKMdv5e2zvKb97+qn+TYipRgfV7GZrI+p8RsZ+6qnnBGtzvayz6QRjjPg0EgJROfxLRIQBCUSg4+uucGr8TgUOyNPc2ao3O+mCwzKmnKZOagsx7AD6dtmI4sZqceb22pkZjEL26oRUzDDBLGFsy1zP3f+wHdC+Pt0OY5WC4DCDtP7uZ+Mxvr/8XxAV59kM7BufXJvtIu//3RuM4Md65EKgGnhx7B1Gd9SjDcxwmJBrfkJNMJxKG5WwOan9geZYi9S/Li2qt1LhkT/HWvWwJdahL1IfZEdH3N83yX8ebSIFo0OPi/IDB9FaZDUbbqypAwyE1Z19iBZF9V6+gxOV8Ia1xBt9K9rZ4oSp5gei53Fqq5nJKXuWIvxNAXaceae89CCDFS2M1+VyLr4if35czEzek17/E2Em93FBaMh8THq7b4c98z1OVXazX0X/hiS9fI7Buc+9jUF06dm9jDBzJAXdRRMdk2bcVvJ9Si2lxm/ZfxG1SbtDHOxfTVgzin1PW3HedwBh1D2Wtbd2L2gqGJnBkqe6WBKONrLuex+bWBgBJOZqSz9nJ0BrL29EntcuefVSFsTlYlAj3H3Fz/wPiS5TzfcB5ru52P5gI4+yc8o2IlJ793HmKktCMa3eFuu4gVFlazN2zLSVjv1GVsRKSbbW+OPdOsyvm0su3fPvTJZpAhpaxxfBH3lhBYhh+51gi+lEgMLDTy7XsaeLN1IBAoAvIvYEqqtT6oC/F1gaJ81IiKAJ04DdEKyJKd+uaReNwbz7wjyHJTNYTpbH8A7TmOmxac2q2VZPiWbezI1AMC7zOhX+5yC4yoNA4C3GffsI0U5E0CVxkyLxfaYmhpMj5RlVXtDyLQAAAAAAMAx0EG0AAAAAAAAvDnaB+IDAAAAAAC8QRAtAAAAAADQaxAtAAAAAADQaxAtAAAAAADQaxAtAAAAAADQaxAtAAAAAADQaxAtAAAAAADQaxAtAAAAAADQaxAtAAAAAADQaxAtAAAAAADQa04catzriGfPnrlXAAAAAAAAMWfPnnWvjp9a0QIAAAAAANAHKA8DAAAAAIBeg2gBAAAAAIBeg2gBAAAAAIAeo9T/ASsBQOohEJCxAAAAAElFTkSuQmCC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS_VpCGTsCvk",
        "outputId": "8f41cdd3-3b95-4c21-b425-67a58504688b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.3)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d8sislhRsl-i"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EJJUFA1Jsl8E"
      },
      "outputs": [],
      "source": [
        "with open(\"openai_key.txt\", \"r\") as file:\n",
        "    openai.api_key = file.read().strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t19CYUkrV2q1"
      },
      "source": [
        "# Set up the actual AG of RAG:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4Mcp72sVfyT"
      },
      "source": [
        "##Set up for local model (for inspiration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUqsL9nOoblj"
      },
      "outputs": [],
      "source": [
        "def is_relevant(query_results, threshold=0.5):\n",
        "    \"\"\"Check if the top retrieved documents are relevant based on a threshold.\n",
        "    query_results: Explanation\n",
        "    threshold: This is used to determine relevancy of stored texts in relation to the query\n",
        "    \"\"\"\n",
        "    # Assume relevance based on the highest score among the top results\n",
        "    if not query_results[\"matches\"]:\n",
        "        return False\n",
        "    return query_results[\"matches\"][0][\"score\"] >= threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxO_D7CmzpaZ"
      },
      "source": [
        "This function does the same as the stuff for the API, it's just put together into one massive clump of code. This is one of my bad habits, and do not do this for your own sanity. It might sound cool to build a massive function that does it all, but it's more efficient to build a lot of functions that do the same, and then put them together afterwards if need be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeLTMHondsv6"
      },
      "outputs": [],
      "source": [
        "def generate_response_with_retrieved_documents(query, top_k=5):\n",
        "    # Generate query embedding and convert it from ndarray to list\n",
        "    query_embedding = sentence_transformer.encode([query])[0].astype('float32').tolist()\n",
        "\n",
        "    # Retrieve top K similar documents from Pinecone using keyword arguments\n",
        "    query_results = index.query(vector=query_embedding, top_k=top_k)\n",
        "    if not is_relevant(query_results, threshold=0.5):\n",
        "        # Clear and directive GPT prompt\n",
        "        suggestion_prompt = f\"Considering the topic '{query}', create a python-format list related Wikipedia article titles separated by commas. Focus strictly on generating relevant titles without additional explanations.\"\n",
        "        suggested_titles = llm(suggestion_prompt)\n",
        "        print(suggested_titles)\n",
        "        # Extract and process potential titles from the GPT output\n",
        "        potential_titles = [title.strip() for title in suggested_titles[0]['generated_text'].split(',') if title.strip() and \" \" in title and not title.lower().startswith('considering the topic')]\n",
        "\n",
        "        # Update the database with these potential titles\n",
        "        for title in potential_titles:\n",
        "            if title and title.isprintable() and len(title.split()) > 1:  # Basic checks for valid titles\n",
        "                # Additional checks to filter out phrases from the prompt\n",
        "                if not any(phrase in title.lower() for phrase in ['list only the titles', 'related to the topic']):\n",
        "                    add_or_update_page_in_pinecone(title)\n",
        "\n",
        "        # Re-query Pinecone after updating using keyword arguments\n",
        "        query_results = index.query(vector=query_embedding, top_k=top_k)\n",
        "\n",
        "    # Fetch the contents of the matched documents\n",
        "    matched_titles = [match[\"id\"] for match in query_results[\"matches\"]]\n",
        "    documents = []\n",
        "    for title in matched_titles:\n",
        "        fetch_response = index.fetch(ids=[title])  # Correct method to fetch data\n",
        "        if title in fetch_response['vectors']:\n",
        "            documents.append(fetch_response['vectors'][title].get('metadata', {}).get('content', ''))\n",
        "\n",
        "    # Combine the documents into a single text\n",
        "    combined_documents = \" \".join(documents)\n",
        "\n",
        "    # Use GPT to generate a response based on the combined documents\n",
        "    input_for_gpt = f\"Based on the provided documents, answer the question: '{query}'. {combined_documents}\"\n",
        "    generated_texts = llm(input_for_gpt)\n",
        "    return generated_texts[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EURL09sWripB"
      },
      "source": [
        "##Setup for API instead (because Google restrictions suck):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrqbhllgCrcU"
      },
      "source": [
        "This function, **retrieve_similar_documents()**, is designed to find documents that are similar to a given query within a vector database (like Pinecone).\n",
        "\n",
        "It takes the users question, turns it into embeddings using the sentence_transformer and uses that embedding vector to query the vector databases for the top 5 matching documents based on cosine similarity (which we set as our search criteria when we created the index)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YZ76ne8HzCjm"
      },
      "outputs": [],
      "source": [
        "def retrieve_similar_documents(query, top_k=5):\n",
        "    query_embedding = sentence_transformer.encode([query])[0].astype('float32').tolist()\n",
        "    #print(f\"Query Embedding: {query_embedding}\")  # Debug: Check the query embedding\n",
        "\n",
        "    query_results = index.query(vector=query_embedding, top_k=top_k)\n",
        "    #print(f\"Query Results: {query_results}\")  # Debug: Check the returned results\n",
        "\n",
        "    return query_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNE6eEidD-mj"
      },
      "source": [
        "This function, check_relevance, is designed to evaluate whether the most relevant document retrieved by the retrieve_similar_documents function is indeed relevant based on a predefined threshold (basically a nice little double-check):\n",
        "\n",
        "It checks if there are any matches returned by the query. If there are no matches, it prints \"No matches found.\" and returns False, indicating no relevant documents were found.\n",
        "\n",
        "If there are matches, the function retrieves the highest similarity score from the returned results, which is the score of the most similar document to the query.\n",
        "\n",
        "Finally, it evaluates whether the top match's score exceeds the specified threshold (default is 0.5). If the score is higher than the threshold, it implies the document is relevant to the query and returns True. Otherwise, it returns False. This helps in deciding whether to proceed with using the retrieved documents for further processing or response generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pVp_KZjnzChv"
      },
      "outputs": [],
      "source": [
        "def check_relevance(query_results, threshold=0.5):\n",
        "    if not query_results[\"matches\"]:\n",
        "        print(\"No matches found.\")  # Debug: No matches\n",
        "        return False\n",
        "    top_score = query_results[\"matches\"][0][\"score\"]\n",
        "    #print(f\"Top match score: {top_score}, Threshold: {threshold}\")  # Debug: Compare top score and threshold\n",
        "    return top_score >= threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnIT3NQ2Ee9T"
      },
      "source": [
        "I know this is a bit of a mouthful, but here goes:<br><br>\n",
        "This function, **generate_and_format_titles()**, automates the process of generating a list of Wikipedia article titles related to a given query using an OpenAI model:\n",
        "\n",
        "**Setting Up Prompt:** It constructs a prompt instructing the language model to generate a list of Wikipedia article titles related to the given query. The instruction specifies the format expected: a Python list of strings without leading hyphens, numbers, or unnecessary characters.\n",
        "\n",
        "**Requesting OpenAI API:** The function then sends this prompt to the specified OpenAI model (default gpt-3.5-turbo-instruct) to generate a response. Parameters like max_tokens, temperature, top_p, frequency_penalty, and presence_penalty are set to guide the response generation.\n",
        "\n",
        "**Processing the Response:** The function strips any leading or trailing whitespace from the response text and attempts to evaluate it directly into a Python list using eval(). This is a critical step but comes with a warning: using eval() can be risky as it will execute the string as Python code, which can be dangerous if the string contains malicious code.\n",
        "\n",
        "**Validation and Return:** It checks whether the evaluated output is indeed a list. If so, it returns this list of titles. If not, it raises a ValueError indicating that the output is not in the expected format. In the case of a syntax error during evaluation (which could happen if the model's output isn't a valid Python expression), it prints an error message and returns an empty list.\n",
        "\n",
        "**Handling Risks:** The comments highlight the inherent risk of using eval() and suggest that if direct evaluation is considered too dangerous or if the output format is too varied and unpredictable, an alternative approach should be considered, such as manually parsing the string into a list.\n",
        "\n",
        "This function is pivotal in translating natural language queries into specific, structured requests for information that can be further processed or queried against a database or API, like fetching detailed content from Wikipedia for those titles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "erzxqXFKzCfk"
      },
      "outputs": [],
      "source": [
        "def generate_and_format_titles(query, model='gpt-3.5-turbo-instruct'):\n",
        "    suggestion_prompt = f\"\"\"\n",
        "    Given the topic '{query}', generate a list of related Wikipedia article titles in Python list format, with each title as a string. Ensure the titles are relevant and formatted correctly within the list without leading hyphens, numbers, or unnecessary characters. For example: ['Title1', 'Title2', 'Title3'].\n",
        "    \"\"\"\n",
        "\n",
        "    suggested_titles_response = openai.completions.create(\n",
        "        model=model,\n",
        "        prompt=suggestion_prompt,\n",
        "        max_tokens=1024,\n",
        "        temperature=0.0,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    #print(f'RESPONSE BEFORE FORMATTING: {suggested_titles_response.choices[0].text}') #Debugging print()\n",
        "\n",
        "    # Extracting the text from the response\n",
        "    suggested_titles_text = suggested_titles_response.choices[0].text.strip()\n",
        "\n",
        "    # Directly evaluating the string to convert to Python list\n",
        "\n",
        "    # Because the output can vary a bit (these models have creative temperament).\n",
        "    # I've found a way to adress format issues. eval() takes strings and executes it as code\n",
        "    # THIS IS PRETTY DANGEROUS, AS IT WILL EXECUTE ANY CODE FORMATTED AS A STRING!\n",
        "    try:\n",
        "        formatted_titles = eval(suggested_titles_text)\n",
        "        #print(f'Formatted Titles: {formatted_titles}')  # Debug: Check the evaluated result\n",
        "        if isinstance(formatted_titles, list):  # Ensure the output is a list\n",
        "            return formatted_titles\n",
        "        else:\n",
        "            raise ValueError('Output is not a list')\n",
        "    except SyntaxError:\n",
        "        print('Failed to parse the response as a Python list. Check the output format dumb-dumb.') #This is a so-called Exception Error. This is what Python spits at you if you're a bad coder!\n",
        "        return []  # Return an empty list or handle the error as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8cIJRAwFeUo"
      },
      "source": [
        "The **update_and_requery_database()** function updates a Pinecone vector database with new or modified Wikipedia page titles and re-queries the database to retrieve relevant documents:\n",
        "\n",
        "**Updating Titles:** Iterates through the list of formatted titles from Wikipedia. For each title, it checks if the title is printable and consists of more than one word. If these conditions are met, it updates or adds the title and its associated information to the Pinecone index using the add_or_update_page_in_pinecone function.\n",
        "\n",
        "**Skipping Invalid Titles:** If a title does not meet the conditions, it skips the update for that title and prints a message indicating this.\n",
        "\n",
        "**Requery Database:** After updating, it re-queries the Pinecone index with the original query embedding to fetch the top documents that are now most relevant to the query after the update. The number of top documents to return is specified by top_k.\n",
        "\n",
        "**Debugging and Return:** Prints the results of the re-query to help with debugging and returns these results for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "A-0jND6TzCdY"
      },
      "outputs": [],
      "source": [
        "def update_and_requery_database(formatted_titles, query_embedding, top_k=5):\n",
        "    for title in formatted_titles:\n",
        "        print(f\"Updating title: {title}\")  # Debug: Which titles are being updated\n",
        "        if title and title.isprintable() and len(title.split()) > 1:\n",
        "            add_or_update_page_in_pinecone(title)\n",
        "        else:\n",
        "            print(f\"Skipped adding/updating title: {title}\")  # Debug: Which titles are skipped\n",
        "    query_results = index.query(vector=query_embedding, top_k=top_k)\n",
        "    #print(f\"Requery Results: {query_results}\")  # Debug: Check the new query results\n",
        "    return query_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XDNHa-EGh5e"
      },
      "source": [
        "The generate_final_response function constructs and retrieves a final answer for a query based on documents retrieved from the Pinecone index:\n",
        "\n",
        "Extract Matched Titles: Takes the titles from the query results that match the user's query.\n",
        "\n",
        "Fetch Document Content: For each title, retrieves the corresponding document content from the Pinecone index.\n",
        "\n",
        "Combine Documents: Concatenates the contents of these documents into a single string, ensuring it stays within a manageable length (e.g., to respect the token limits of the language model).\n",
        "\n",
        "Construct Prompt: Creates a prompt for the OpenAI model that includes the question (query) and the combined documents. The prompt instructs the model to provide an answer based on the given information.\n",
        "\n",
        "Generate Response: Sends the constructed prompt to the specified OpenAI model (e.g., 'gpt-3.5-turbo-instruct') and receives the generated answer.\n",
        "\n",
        "Return Final Answer: Extracts and returns the text of the response generated by the model, which should be an answer to the user's question based on the information from the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nkIkSdNGzCbM"
      },
      "outputs": [],
      "source": [
        "def generate_final_response(query_results, query, model='gpt-3.5-turbo-instruct'):\n",
        "    # Extract matched titles from the query results\n",
        "    matched_titles = [match[\"id\"] for match in query_results[\"matches\"]]\n",
        "\n",
        "    # Initialize a list to hold the documents' full contents or relevant sections\n",
        "    documents = []\n",
        "\n",
        "    # Fetch each document by title and append its content to the documents list\n",
        "    for title in matched_titles:\n",
        "        fetch_response = index.fetch(ids=[title])\n",
        "        if title in fetch_response['vectors']:  # Ensure the title exists in the fetch response\n",
        "            # Append the full content or a relevant section, not just the summary\n",
        "            documents.append(fetch_response['vectors'][title]['metadata'].get('content', ''))\n",
        "\n",
        "    # Combine the documents into a single string, considering the token limit\n",
        "    combined_documents = \" \".join(documents)[:2000]  # Adjust the slice as needed based on token limits\n",
        "\n",
        "    # Construct the input prompt for the OpenAI model\n",
        "    input_for_gpt = f\"Based on the provided documents, answer the question: '{query}'.\\n\\n{combined_documents}\"\n",
        "\n",
        "    # Generate the response using the OpenAI model\n",
        "    generated_texts_response = openai.completions.create(\n",
        "        model=model,\n",
        "        prompt=input_for_gpt,\n",
        "        max_tokens=1024,\n",
        "        temperature=0.7,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    # Extract and return the text of the generated response\n",
        "    generated_text = generated_texts_response.choices[0].text.strip()\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8gX2Ng_tSAE"
      },
      "source": [
        "# Let's try this monstrosity out:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UZs8wUw0SI6"
      },
      "source": [
        "## API test setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHP0W9GtHIju"
      },
      "source": [
        "First we test the flow, using the individual functions, to make sure we get the results we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "P4UwHgx_1_vL"
      },
      "outputs": [],
      "source": [
        "# Working prompt\n",
        "query = \"What are the potential impacts of artificial intelligence on the future job market?\"\n",
        "# Non-working prompt\n",
        "#query = \"What is the meaning of life?\"\n",
        "# This is likely due to some of the logic I've set up here. The logic should probably be;\n",
        "# IF no similar Titles are found on Wiki, simply continue and use the highest similarities in the pinecone index\n",
        "# OR!!!! maybe Wikipedia doesn't care about the meaning of life. THOSE BASTARDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG4AOF-yKSMa"
      },
      "source": [
        "Judging by the output of this cell, it is definitely a logic error somewhere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCEa5Gu616B0"
      },
      "outputs": [],
      "source": [
        "# Step 1: Generate titles related to the query\n",
        "suggested_titles = generate_and_format_titles(query, model='gpt-3.5-turbo-instruct')  # Adjust the model as needed\n",
        "\n",
        "# Step 2: Fetch content for each title from Wikipedia\n",
        "all_content_data = []\n",
        "for title in suggested_titles:\n",
        "    content_data = fetch_relevant_wikipedia_content(title)\n",
        "    all_content_data.extend(content_data)  # Aggregate content data from all titles\n",
        "\n",
        "# Step 3: Update Pinecone index with the fetched content\n",
        "for article_data in all_content_data:\n",
        "    add_or_update_page_in_pinecone(article_data)\n",
        "\n",
        "# Step 4: Retrieve similar documents from Pinecone based on the original query\n",
        "query_results = retrieve_similar_documents(query, top_k=5)\n",
        "\n",
        "# Step 5: Check if the top result is relevant (customize threshold as needed)\n",
        "is_relevant = check_relevance(query_results, threshold=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XrocJff5kAx",
        "outputId": "cfce8f06-fa33-4335-a8b4-45108ceb3802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Response: The potential impacts of artificial intelligence on the future job market are complex and multifaceted. On one hand, AI has the potential to create new job opportunities and increase efficiency in many industries. On the other hand, it could also lead to job displacement and a widening income gap between those who have the skills to work with AI and those who do not.\n",
            "\n",
            "One potential impact of AI on the job market is the automation of tasks that are currently performed by humans. This could lead to a decrease in demand for certain types of jobs, particularly those that involve repetitive or routine tasks. For example, AI could replace human workers in industries such as manufacturing, transportation, and customer service.\n",
            "\n",
            "However, at the same time, AI is also expected to create new job opportunities in fields such as data analysis, machine learning, and AI development. As AI technology continues to advance, there will be a growing need for workers with specialized skills and knowledge to develop, implement, and maintain these systems.\n",
            "\n",
            "Another potential impact of AI on the job market is the transformation of existing jobs. As AI technology becomes more advanced, it has the potential to enhance and augment human capabilities, rather than replace them entirely. This could lead to a shift in job roles and responsibilities, as workers will need to adapt to working alongside AI systems.\n",
            "\n",
            "In addition, there are concerns that AI could exacerbate existing inequalities in the job market. The development and implementation of AI systems require significant resources and expertise, which may only be available to certain individuals or organizations. This could lead to a widening income gap between those who have access to AI technology and those who do not.\n",
            "\n",
            "The impact of AI on the job market also raises questions about the future of education and training. As AI technology becomes more prevalent, there will be a growing demand for workers with skills in data analysis, programming, and other technical areas. This could require a shift in the education system to ensure that workers are equipped with the necessary skills to succeed in an AI-driven job market.\n",
            "\n",
            "Overall, the potential impacts of AI on the future job market are significant and far-reaching. While it has the potential to create new job opportunities and increase efficiency, it also poses challenges and raises important questions about the future of work and the workforce. As AI technology continues to advance, it will be crucial for individuals, organizations, and governments to adapt and prepare for the changes it brings.\n"
          ]
        }
      ],
      "source": [
        "if is_relevant:\n",
        "    final_response = generate_final_response(query_results, query, model='gpt-3.5-turbo-instruct')  # Adjust the model as needed\n",
        "    print(\"Final Response:\", final_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH1uwb4iHS70"
      },
      "source": [
        "And now we can combine all of the functions into a single flowing function, that we can use to automate the whole process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "D7QcuCu0HaYY"
      },
      "outputs": [],
      "source": [
        "def ASK_ME_MF(query, model='gpt-3.5-turbo-instruct', top_k=5, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Executes the complete workflow from generating titles related to the query,\n",
        "    fetching content, updating the Pinecone index, retrieving and checking similar documents,\n",
        "    and generating a final response based on the query.\n",
        "\n",
        "    Args:\n",
        "    query (str): The user's query or question.\n",
        "    model (str): The OpenAI model to be used.\n",
        "    top_k (int): The number of top documents to retrieve from Pinecone.\n",
        "    threshold (float): The relevance score threshold to consider a document relevant.\n",
        "\n",
        "    Returns:\n",
        "    str: The final response generated based on the query and the documents.\n",
        "    \"\"\"\n",
        "    # Step 1: Generate titles related to the query\n",
        "    suggested_titles = generate_and_format_titles(query, model=model)\n",
        "\n",
        "    # Step 2: Fetch content for each title from Wikipedia\n",
        "    all_content_data = []\n",
        "    for title in suggested_titles:\n",
        "        content_data = fetch_relevant_wikipedia_content(title)\n",
        "        all_content_data.extend(content_data)  # Aggregate content data from all titles\n",
        "\n",
        "    # Step 3: Update Pinecone index with the fetched content\n",
        "    for article_data in all_content_data:\n",
        "        add_or_update_page_in_pinecone(article_data)\n",
        "\n",
        "    # Step 4: Retrieve similar documents from Pinecone based on the original query\n",
        "    query_results = retrieve_similar_documents(query, top_k=top_k)\n",
        "\n",
        "    # Step 5: Check if the top result is relevant (customize threshold as needed)\n",
        "    is_relevant = check_relevance(query_results, threshold=threshold)\n",
        "\n",
        "    # Step 6: Generate and return the final response if relevant\n",
        "    if is_relevant:\n",
        "        final_response = generate_final_response(query_results, query, model=model)\n",
        "        return final_response\n",
        "    else:\n",
        "        return \"No relevant documents found for the query.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km5m5CtHH7q_"
      },
      "outputs": [],
      "source": [
        "print(ASK_ME_MF(\"What are the potential impacts of artificial intelligence on the future job market?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BocW1yH1K0Lt"
      },
      "source": [
        "And that's how you do RAG with GPT models (and possibly get blocked from using an API)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_zvBTCf0Pr2"
      },
      "source": [
        "##Local test setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms2hV9RrAw5l"
      },
      "source": [
        "I know what you're thinking: A 3 line setup? Hell yes! <br>\n",
        "**HELL NO!** This is such a bad idea, since it makes it incredibly annoying to find bugs. Never ever ever do what I do and create big walls of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJHmHqBuejFP"
      },
      "outputs": [],
      "source": [
        "# Example question for local model setup:\n",
        "question = \"What are the effects of global warming?\"\n",
        "response = generate_response_with_retrieved_documents(question, top_k=5)\n",
        "print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcYW9ZR4VhGb"
      },
      "source": [
        "#Setting up GPT locally as the logic engine **(Full functionality not tested due to GPU backend limits in Colab)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okzn0kEsBMCf"
      },
      "source": [
        "Let's set up our backend GPT that will be function as the logic engine behind the app. <br>\n",
        "In this case, I've professionally borrowed some of Hamids code to setup the LLM pipeline. Code I got from this notebook: <br>\n",
        "https://colab.research.google.com/github/aaubs/ds-master/blob/main/notebooks/M3_3_NLG_4_Solutions.ipynb#scrollTo=mEC0V1DcpG1U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p07nM-aONMl"
      },
      "source": [
        "**Once you've set the local model up to generate basic text, it's essentially the same workflow as with the API. The only real difference in terms of functionality is that we'll call the model using llm() instead of setting up API calls.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aM7MHnIjMy0L"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate --q\n",
        "!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off\n",
        "!pip install -qqq optimum==1.13.1 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGzAVKDWlsdo"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvFSJvzyMvyV"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline, GenerationConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffOmod8WQ6tF"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "f2d6e68cba4d409fad4c8f4ec416deb9",
            "11daa53505f54f31b39fb0af8d6e3e65",
            "e6a8ea4d894249878de9b03f2e6f3445",
            "e6ec0030096143cb939e906e84188bb5",
            "ea44a6dc95ff4ae2952081be74e955eb",
            "a9c91532f0094de5b8082e784a2df0b3",
            "fd2ddf1781aa4d168243bfff063b429c",
            "d5d2155a3b1d433783b79c9783699675",
            "6b7290f26f334c268284202bbb7f339b",
            "58c7c786bd9d47dfaf212cd3b954624a",
            "9db0ab5fa95e448a85c6ef30e0f3541e",
            "afc9039197aa415f889c6d1827c1e334",
            "06f9355bb0d64489a53ff744f9e5268c",
            "7a4a4b61d3fc4169a3ea664eaf3d53f5",
            "ca2a1f515e9449e380aa6f9d273a5a93",
            "b674551ce87841f480996bb8b53bbb22",
            "486e4eabb1bf41e8bf56600d63863109",
            "b92323af610840368f5a0640bad4183c",
            "cecfe2aa65d549048aa70a238d040ccb",
            "593d4b08fb804f7b8e9d8f1bdb8625a6",
            "d0f2a1effdf946dd8ab51f9f3f514997",
            "ca07e8382f8b4546a7795abedb4ddcee"
          ]
        },
        "id": "4ndaG-gaejI4",
        "outputId": "159dfed6-8c9b-4bdb-ff56-6604b1bb1f86"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2d6e68cba4d409fad4c8f4ec416deb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afc9039197aa415f889c6d1827c1e334",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "MODEL_NAME = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Create a configuration for text generation based on the specified model name\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Set the maximum number of new tokens in the generated text to 1024.\n",
        "# This limits the length of the generated output to 1024 tokens.\n",
        "generation_config.max_new_tokens = 1024\n",
        "\n",
        "# Set the temperature for text generation. Lower values (e.g., 0.0001) make output more deterministic, following likely predictions.\n",
        "# Higher values make the output more random.\n",
        "generation_config.temperature = 0.0001\n",
        "\n",
        "# Set the top-p sampling value. A value of 0.95 means focusing on the most likely words that make up 95% of the probability distribution.\n",
        "generation_config.top_p = 0.95\n",
        "\n",
        "# Enable text sampling. When set to True, the model randomly selects words based on their probabilities, introducing randomness.\n",
        "generation_config.do_sample = True\n",
        "\n",
        "# Set the repetition penalty. A value of 1.15 discourages the model from repeating the same words or phrases too frequently in the output.\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "\n",
        "# Create a text generation pipeline using the initialized model, tokenizer, and generation configuration\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "# Create a LangChain pipeline that wraps the text generation pipeline and set a specific temperature for generation\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rsTsUBNaVxMc",
        "TlpU5xGxXYzQ",
        "24SIyRJiZQ6n",
        "xuGE9nu2ewoF",
        "LF-GQHWLezYB",
        "Xkag1Xvae6tu",
        "t19CYUkrV2q1"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06f9355bb0d64489a53ff744f9e5268c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_486e4eabb1bf41e8bf56600d63863109",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b92323af610840368f5a0640bad4183c",
            "value": "generation_config.json:â€‡100%"
          }
        },
        "11daa53505f54f31b39fb0af8d6e3e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9c91532f0094de5b8082e784a2df0b3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fd2ddf1781aa4d168243bfff063b429c",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "486e4eabb1bf41e8bf56600d63863109": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58c7c786bd9d47dfaf212cd3b954624a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "593d4b08fb804f7b8e9d8f1bdb8625a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b7290f26f334c268284202bbb7f339b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a4a4b61d3fc4169a3ea664eaf3d53f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cecfe2aa65d549048aa70a238d040ccb",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_593d4b08fb804f7b8e9d8f1bdb8625a6",
            "value": 137
          }
        },
        "9db0ab5fa95e448a85c6ef30e0f3541e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9c91532f0094de5b8082e784a2df0b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afc9039197aa415f889c6d1827c1e334": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06f9355bb0d64489a53ff744f9e5268c",
              "IPY_MODEL_7a4a4b61d3fc4169a3ea664eaf3d53f5",
              "IPY_MODEL_ca2a1f515e9449e380aa6f9d273a5a93"
            ],
            "layout": "IPY_MODEL_b674551ce87841f480996bb8b53bbb22"
          }
        },
        "b674551ce87841f480996bb8b53bbb22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b92323af610840368f5a0640bad4183c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca07e8382f8b4546a7795abedb4ddcee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca2a1f515e9449e380aa6f9d273a5a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0f2a1effdf946dd8ab51f9f3f514997",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ca07e8382f8b4546a7795abedb4ddcee",
            "value": "â€‡137/137â€‡[00:00&lt;00:00,â€‡10.4kB/s]"
          }
        },
        "cecfe2aa65d549048aa70a238d040ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0f2a1effdf946dd8ab51f9f3f514997": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5d2155a3b1d433783b79c9783699675": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6a8ea4d894249878de9b03f2e6f3445": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5d2155a3b1d433783b79c9783699675",
            "max": 3896726136,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b7290f26f334c268284202bbb7f339b",
            "value": 3896726136
          }
        },
        "e6ec0030096143cb939e906e84188bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58c7c786bd9d47dfaf212cd3b954624a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9db0ab5fa95e448a85c6ef30e0f3541e",
            "value": "â€‡3.90G/3.90Gâ€‡[00:36&lt;00:00,â€‡134MB/s]"
          }
        },
        "ea44a6dc95ff4ae2952081be74e955eb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2d6e68cba4d409fad4c8f4ec416deb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11daa53505f54f31b39fb0af8d6e3e65",
              "IPY_MODEL_e6a8ea4d894249878de9b03f2e6f3445",
              "IPY_MODEL_e6ec0030096143cb939e906e84188bb5"
            ],
            "layout": "IPY_MODEL_ea44a6dc95ff4ae2952081be74e955eb"
          }
        },
        "fd2ddf1781aa4d168243bfff063b429c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
